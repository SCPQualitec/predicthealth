{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To-Do May 16\n",
    "\n",
    "## HMM!\n",
    "\n",
    "## Port PCA to csv for use in R, Bayes, etc\n",
    "\n",
    "## Find out how many individuals are represented in correct vs incorrect predictions\n",
    "Eg. do we get some users who always show up in false neg or false pos, or in true pos?  \n",
    "This might be especially useful for the url and created date analyses, where we don't know the extent to which a subset of usernames might be driving correct classification.  \n",
    "for those in false pos, look at the actual posts (might be easier for twitter), and maybe even have mturk rate whether they seem depressed or not. are we finding new depressed cases, or are we just wrong?  \n",
    "\n",
    "## Restrict username analysis to only those individuals with a minimum number of days represented in their observations \n",
    "Alternately, min number of posts   \n",
    "See R code notes for more. (bayes.R)\n",
    "\n",
    "## Move R code to Jupyter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data/LIWC/LIWC2007_English100131_words.dic\n",
      "loading LIWC with stopVal=0.5, for 906 words\n",
      "loading data/LIWC/LIWC2007_English100131_words.dic\n",
      "loading LIWC with stopVal=0.0, for 4483 words\n",
      "loading LabMT with stopVal=1.0, for 3731 words\n",
      "loading ANEW with stopVal=1.0, for 765 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "%matplotlib inline\n",
    "\n",
    "from bgfunc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SQLite database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dbfile = 'project_may9_649pm.db'\n",
    "conn = sqlite3.connect(dbfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis parameters \n",
    "\n",
    "Which condition are we studying?  Are there any cutoffs based on testing?  \n",
    "What sort of analyses or models do we want to run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "platform = 'tw' # ig = instagram, tw = twitter\n",
    "condition = 'depression' # depression, pregnancy, ptsd, cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "specs = analysis_specifications(platform, condition)\n",
    "\n",
    "platform_long = specs['plong'][platform]\n",
    "gb_types = specs['gb_types'][platform]\n",
    "fields = specs['fields'][platform] \n",
    "test_name = specs['test_name'][condition]\n",
    "test_cutoff = specs['test_cutoff'][condition]\n",
    "photos_rated = specs['photos_rated'][condition]\n",
    "has_test = specs['has_test'][condition]\n",
    "\n",
    "clfs = ['lr','rf'] # lr = logistic regression, rf = random forests, svc = support vector\n",
    "periods = ['before','after']\n",
    "turn_points = ['from_diag','from_susp']\n",
    "\n",
    "impose_test_cutoff = True # do we want to limit target pop based on testing cutoff (eg. cesd > 21)?\n",
    "\n",
    "report_sample_size = False # simple reporting feature\n",
    "\n",
    "load_from_pickle = True # loads entire data dict, including masters, from pickle file\n",
    "\n",
    "final_pickle = True # pickles entire data dict after all masters are created\n",
    "\n",
    "populate_wordfeats_db = False # generates word features from reagan code\n",
    "\n",
    "run_master = True \n",
    "run_subsets = True\n",
    "run_before_after = False\n",
    "run_separate_pca = False\n",
    "\n",
    "action_params = {\n",
    "    'create_master': True, \n",
    "    'save_to_file' : False, \n",
    "    'density' : False, \n",
    "    'ml' : False, \n",
    "    'nhst' : False, \n",
    "    'corr' : False, \n",
    "    'print_corrmat' : False,\n",
    "    'tall_plot': True\n",
    "}\n",
    "\n",
    "params = define_params(condition, test_name, test_cutoff, impose_test_cutoff,\n",
    "                       platform, platform_long, fields, photos_rated, has_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# printout showing sample sizes for target and control groups\n",
    "if report_sample_size:\n",
    "    report_sample_sizes(params, conn, condition, platform_long, test_cutoff, test_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ready data, or prepare raw data\n",
    "\n",
    "Set load_from_pickle to determine action here.  \n",
    "\n",
    "In case you don't have a pickled data dict, or if you want to make a new one, the next block will:\n",
    "\n",
    "- Pulls data from db\n",
    "- Aggregates in buckets (day, week, user)\n",
    "- Creates before/after diag/susp date subsets along with whole\n",
    "\n",
    "Otherwise we load existing cleaned/aggregated data from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if load_from_pickle:\n",
    "    data = pickle.load( open( \"{cond}_{pl}_data.p\".format(cond=condition,pl=platform), \"rb\" ) )\n",
    "    \n",
    "else:\n",
    "    data = make_data_dict(params, condition, test_name, conn)\n",
    "    prepare_raw_data(data, platform, params, conn, gb_types, condition, periods, turn_points)\n",
    "    pickle.dump( data, open( \"{cond}_{pl}_data.p\".format(cond=condition,pl=platform), \"wb\" ) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This next section generates word features from Andy Reagan's code\n",
    "\n",
    "Only set populate_wordfeats_db = True if you need to redo the features for some reason!\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if populate_wordfeats_db:\n",
    "    create_word_feats_wrapper(['target','control'], gb_types, data, condition, conn, \n",
    "                              write_to_db=True, testing=False)                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Construct master dataset & run analyses\n",
    "\n",
    "Possible actions:\n",
    "- generate master data\n",
    "- save to disk\n",
    "- plot target vs control densities for each variable\n",
    "- correlation plot\n",
    "- ML modeling\n",
    "- NHST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merge to master: MAIN created_date\n",
      "master created_date shape: (34676, 86)\n",
      "\n",
      "\n",
      "Merge to master: MAIN weekly\n",
      "master weekly shape: (10622, 86)\n",
      "\n",
      "\n",
      "Merge to master: MAIN user_id\n",
      "master user_id shape: (190, 78)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if run_master:\n",
    "    master = data['master']\n",
    "    target = data['target']['gb']\n",
    "    control = data['control']['gb'] \n",
    "    report = 'MAIN'\n",
    "\n",
    "    if action_params['create_master']:\n",
    "        master['model'] = {}\n",
    "\n",
    "    for gb_type in gb_types:\n",
    "\n",
    "        master_actions(master, target, control, condition, platform, \n",
    "                       params, gb_type, report, action_params, clfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset master actions\n",
    "\n",
    "Same as above block, but for subsets, eg. target before diag_date vs controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporting for: SUBSETS\n",
      "Period: BEFORE  Focus: FROM_DIAG  Groupby: CREATED_DATE\n",
      "\n",
      "Merge to master: before from_diag created_date\n",
      "master created_date shape: (25948, 86)\n",
      "\n",
      "Reporting for: SUBSETS\n",
      "Period: BEFORE  Focus: FROM_DIAG  Groupby: WEEKLY\n",
      "\n",
      "Merge to master: before from_diag weekly\n",
      "master weekly shape: (8420, 86)\n",
      "\n",
      "Reporting for: SUBSETS\n",
      "Period: BEFORE  Focus: FROM_DIAG  Groupby: USER_ID\n",
      "\n",
      "Merge to master: before from_diag user_id\n",
      "master user_id shape: (171, 78)\n",
      "\n",
      "Reporting for: SUBSETS\n",
      "Period: BEFORE  Focus: FROM_SUSP  Groupby: CREATED_DATE\n",
      "\n",
      "Merge to master: before from_susp created_date\n",
      "master created_date shape: (20711, 86)\n",
      "\n",
      "Reporting for: SUBSETS\n",
      "Period: BEFORE  Focus: FROM_SUSP  Groupby: WEEKLY\n",
      "\n",
      "Merge to master: before from_susp weekly\n",
      "master weekly shape: (6997, 86)\n",
      "\n",
      "Reporting for: SUBSETS\n",
      "Period: BEFORE  Focus: FROM_SUSP  Groupby: USER_ID\n",
      "\n",
      "Merge to master: before from_susp user_id\n",
      "master user_id shape: (119, 78)\n",
      "\n",
      "Reporting for: SUBSETS\n",
      "Period: AFTER  Focus: FROM_DIAG  Groupby: CREATED_DATE\n",
      "\n",
      "Merge to master: after from_diag created_date\n",
      "master created_date shape: (26819, 86)\n",
      "\n",
      "Reporting for: SUBSETS\n",
      "Period: AFTER  Focus: FROM_DIAG  Groupby: WEEKLY\n",
      "\n",
      "Merge to master: after from_diag weekly\n",
      "master weekly shape: (8579, 86)\n",
      "\n",
      "Reporting for: SUBSETS\n",
      "Period: AFTER  Focus: FROM_DIAG  Groupby: USER_ID\n",
      "\n",
      "Merge to master: after from_diag user_id\n",
      "master user_id shape: (187, 78)\n",
      "\n",
      "Reporting for: SUBSETS\n",
      "Period: AFTER  Focus: FROM_SUSP  Groupby: CREATED_DATE\n",
      "\n",
      "Merge to master: after from_susp created_date\n",
      "master created_date shape: (21479, 86)\n",
      "\n",
      "Reporting for: SUBSETS\n",
      "Period: AFTER  Focus: FROM_SUSP  Groupby: WEEKLY\n",
      "\n",
      "Merge to master: after from_susp weekly\n",
      "master weekly shape: (7159, 86)\n",
      "\n",
      "Reporting for: SUBSETS\n",
      "Period: AFTER  Focus: FROM_SUSP  Groupby: USER_ID\n",
      "\n",
      "Merge to master: after from_susp user_id\n",
      "master user_id shape: (125, 78)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_pca = False # should models be fit using orthogonal pca components?\n",
    "\n",
    "if run_subsets:\n",
    "    for period in periods:\n",
    "        if action_params['create_master']:\n",
    "            data['master'][period] = {}\n",
    "\n",
    "        for turn_point in turn_points:    \n",
    "            if action_params['create_master']:\n",
    "                data['master'][period][turn_point] = {}\n",
    "\n",
    "            master = data['master'][period][turn_point]\n",
    "            target = data['target'][period][turn_point]['gb']\n",
    "            control = data['control']['gb'] \n",
    "            report = '{} {}'.format(period,turn_point)\n",
    "\n",
    "            if action_params['create_master']:\n",
    "                master['model'] = {}\n",
    "\n",
    "            for gb_type in gb_types:\n",
    "                print 'Reporting for: SUBSETS'\n",
    "                print 'Period: {}  Focus: {}  Groupby: {}'.format(period.upper(), turn_point.upper(), gb_type.upper())\n",
    "                # merge target, control, into master\n",
    "                master_actions(master, target, control, condition,\n",
    "                               platform, params, gb_type, report,\n",
    "                               action_params, clfs, \n",
    "                               use_pca=use_pca) # using PCA!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle entire data dict\n",
    "\n",
    "Set final_pickle = True to save to disk  \n",
    "Note that this is separate from saving individual files to csv, which is controlled by the save_to_file flag in action_params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if final_pickle:\n",
    "    pickle.dump( data, open( \"{cond}_{pl}_data.p\".format(cond=condition,pl=platform), \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Within-target before vs after\n",
    "\n",
    "This compares before/after diag/susp dates within target population.  \n",
    "Basically just a check to see whether the population looks different based on a given change point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# no username gb because you don't have the infrastructure built (you'd need to split bef/aft before the username gb)\n",
    "# but at any rate, this is just a check...and per-username groupby has the lowest sample size anyhow\n",
    "if run_before_after:\n",
    "    for gb_type in ['created_date','weekly']: \n",
    "        before_vs_after(data['target']['gb'], gb_type, platform, condition, params['vars'][platform], action_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "PCA below only runs on master (ie. not before/after diag/susp vs control).\n",
    "\n",
    "Note: You can fold in PCA into the master_actions() sequence, above, by adding the parameter use_pca=True  \n",
    "This will only run PCA and PCA components as predictors, though.  \n",
    "Currently you can't run both PCA and non-PCA when modeling, simulataneously - you did this mainly to cut down on the length of any one given code block output.  \n",
    "You may find PCA particularly helpful for the timeline groups analysis..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_separate_pca:\n",
    "    master = data['master']\n",
    "    report = 'PCA MAIN'\n",
    "\n",
    "    for gb_type in gb_types:\n",
    "\n",
    "        print 'RUNNING PCA: {}'.format(gb_type)\n",
    "        print\n",
    "        model_df = {'name':'Models: {} {}'.format(report, gb_type),\n",
    "                    'unit':gb_type,\n",
    "                    'data':master[gb_type],\n",
    "                    'features':params['vars'][platform][gb_type]['means'],\n",
    "                    'target':'target',\n",
    "                    'platform':platform,\n",
    "                    'tall_plot':action_params['tall_plot']\n",
    "                   }\n",
    "\n",
    "        excluded_set = params['model_vars_excluded'][platform][gb_type]\n",
    "\n",
    "        _, pcafit = make_models(model_df, clf_types=clfs, excluded_set=excluded_set, \n",
    "                                tall_plot=model_df['tall_plot'], use_pca=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
